{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb85fca",
   "metadata": {},
   "source": [
    "CLASS IMBALACE HANDLING TECHNIQUES\n",
    "1.\tEasy Ensemble + SMOTE\n",
    "2.\tCost-Sensitive Learning (Class Weighting)\n",
    "3.\tAnomaly Detection Models\n",
    "    3.1\tIsolation Forest\n",
    "    3.2\tOne-Class SVM\n",
    "    3.3\tAutoencoders\n",
    "4.\tGraph-Based Oversampling (GraphSMOTE, GNNs)\n",
    "5.\tTime-Based Validation + Drift Handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7919b28",
   "metadata": {},
   "source": [
    "Understanding the Imbalance Problem in Blockchain Fraud Detection:\n",
    "In blockchain fraud detection, we face a fundamental challenge: fraudulent transactions (what we're trying to detect) are significantly outnumbered by legitimate transactions. This imbalance creates several problems:\n",
    "1. Models tend to favor the majority class (legitimate transactions) because it improves overall accuracy\n",
    "2. The minority class (fraudulent transactions) doesn't provide enough patterns for the model to learn from\n",
    "3. Traditional algorithms often struggle to identify the rare fraudulent cases\n",
    "4. Performance metrics can be misleading if not carefully selected\n",
    "\n",
    "This severe class imbalance means that even a model that classifies everything as legitimate could achieve high accuracy (e.g., 99% if fraud is only 1% of transactions), while being completely useless for fraud detection. To address this challenge, we need specialized techniques like SMOTE and Easy Ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb4283",
   "metadata": {},
   "source": [
    "1. Easy Ensemble + SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8a4d2",
   "metadata": {},
   "source": [
    "What is SMOTE and How Does It Work?\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is an approach that creates synthetic examples for the minority class rather than simply duplicating existing examples. This provides new information to the model instead of just repeating the same patterns.\n",
    "\n",
    "SMOTE works through the following steps:\n",
    "1. It selects a random example from the minority class (a fraudulent transaction in our case)\n",
    "2. Finds its k nearest neighbors (other similar fraudulent transactions)\n",
    "3. Selects one of these neighbors randomly\n",
    "4. Creates a synthetic example by drawing a line between the original example and the selected neighbor, then placing the new example somewhere along this line\n",
    "\n",
    "In simpler terms, SMOTE creates new fraudulent transaction examples that have characteristics similar to real fraud but aren't exact duplicates. This helps the model learn more robust patterns of what constitutes fraud.\n",
    "\n",
    "The mathematical approach behind SMOTE involves:\n",
    "1. Finding similar fraudulent transactions in feature space\n",
    "2. Creating new examples that maintain the essential characteristics of fraud while introducing some variation\n",
    "3. Generating these examples as a \"convex combination\" of existing examples (essentially, a weighted average of feature values)\n",
    "\n",
    "This approach is particularly effective because it creates plausible new examples rather than arbitrary ones, helping the model build more robust decision boundaries around minority class examples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Understanding Easy Ensemble\n",
    "Easy Ensemble is a technique specifically designed for imbalanced datasets. It creates multiple balanced training sets from the original imbalanced data, trains a separate model on each balanced set, and combines their predictions.\n",
    "\n",
    "The process works as follows:\n",
    "1. It creates multiple balanced training sets by:\n",
    "2. Keeping all examples from the minority class (fraudulent transactions)\n",
    "3. Randomly undersampling the majority class (legitimate transactions) to match the number of minority examples\n",
    "4. Repeating this process multiple times to create different balanced subsets\n",
    "5. For each balanced subset, it trains a model (typically an AdaBoost classifier)\n",
    "6. It combines the predictions from all models to make the final decision\n",
    "\n",
    "Easy Ensemble is essentially a \"bag of balanced boosted learners\" - it creates multiple balanced samples and uses boosting techniques on each sample.\n",
    "\n",
    "The key advantage is that while each individual model only sees a portion of the legitimate transactions, the ensemble as a whole considers all of them. This means we don't lose information from the majority class while still maintaining balance in training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Combining SMOTE and Easy Ensemble for Blockchain Fraud Detection:\n",
    "1. Preparation: First, preprocess your blockchain transaction data (normalizing features, encoding categorical variables, etc.)\n",
    "2. Apply SMOTE: Use SMOTE to generate synthetic examples of fraudulent transactions, increasing their representation in the dataset\n",
    "3. Implement Easy Ensemble: Apply Easy Ensemble on the SMOTE-enhanced dataset to create multiple balanced training sets and train an ensemble of models\n",
    "4. Evaluation: Evaluate the model using appropriate metrics for imbalanced data (precision, recall, F1-score, area under the ROC curve)\n",
    "\n",
    "This combination addresses both the lack of minority examples (using SMOTE) and the imbalanced nature of the dataset (using Easy Ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31065671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Collection and Preprocessing\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your blockchain transaction data\n",
    "# This would include features like transaction amount, time patterns, \n",
    "# network metrics, account behaviors, etc.\n",
    "data = pd.read_csv('blockchain_transactions.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('is_fraud', axis=1)\n",
    "y = data['is_fraud']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Apply SMOTE to Balance the Training Data\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_train_smote))\n",
    "\n",
    "\n",
    "#Here, SMOTE creates synthetic examples of fraudulent transactions based on existing ones. \n",
    "# The sampling_strategy='auto' parameter tells SMOTE to generate enough synthetic examples to make the classes balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c50415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Implement Easy Ensemble Classifier\n",
    "\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Easy Ensemble Classifier\n",
    "# n_estimators: Number of AdaBoost learners in the ensemble\n",
    "# random_state: For reproducibility\n",
    "easy_ensemble = EasyEnsembleClassifier(\n",
    "    n_estimators=10,\n",
    "    random_state=42,\n",
    "    sampling_strategy='auto',\n",
    "    replacement=False,\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Train the model on the SMOTE-enhanced data\n",
    "# You can also apply Easy Ensemble directly without SMOTE\n",
    "# Let's try both approaches\n",
    "model_with_smote = easy_ensemble.fit(X_train_smote, y_train_smote)\n",
    "model_without_smote = easy_ensemble.fit(X_train_scaled, y_train)\n",
    "\n",
    "#The EasyEnsembleClassifier creates multiple balanced training sets by randomly undersampling the majority class.\n",
    "# Each balanced set is used to train an AdaBoost classifier (the default estimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71df288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Make Predictions and Evaluate\n",
    "\n",
    "# Make predictions\n",
    "y_pred_with_smote = model_with_smote.predict(X_test_scaled)\n",
    "y_pred_without_smote = model_without_smote.predict(X_test_scaled)\n",
    "\n",
    "# Get probability predictions (useful for adjusting the threshold)\n",
    "y_pred_proba_with_smote = model_with_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"Performance with SMOTE + Easy Ensemble:\")\n",
    "print(classification_report(y_test, y_pred_with_smote))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_with_smote))\n",
    "\n",
    "print(\"\\nPerformance with Easy Ensemble only:\")\n",
    "print(classification_report(y_test, y_pred_without_smote))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_without_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b253c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Fine-tuning and Optimization\n",
    "\n",
    "# Tuning SMOTE parameters\n",
    "smote_tuned = SMOTE(\n",
    "    sampling_strategy=0.5,  # Adjust ratio of minority to majority (0.5 = 1:2 ratio)\n",
    "    k_neighbors=5,          # Number of nearest neighbors to use\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tuning Easy Ensemble parameters\n",
    "easy_ensemble_tuned = EasyEnsembleClassifier(\n",
    "    n_estimators=50,               # More estimators often improve performance\n",
    "    estimator=None,                # Default is AdaBoostClassifier\n",
    "    sampling_strategy='auto',      # Automatically determine the sampling strategy\n",
    "    replacement=False,             # Sample without replacement\n",
    "    n_jobs=-1,                     # Use all available cores\n",
    "    random_state=42,\n",
    "    verbose=1                      # Show progress during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca685edf",
   "metadata": {},
   "source": [
    "Advanced Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7fcf3a",
   "metadata": {},
   "source": [
    "Customizing the Base Estimator\n",
    "\n",
    "By default, EasyEnsembleClassifier uses AdaBoostClassifier as the base estimator, but you can substitute it with any classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create a custom AdaBoost with specified parameters\n",
    "base_estimator = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Use this custom estimator in the Easy Ensemble\n",
    "easy_ensemble_custom = EasyEnsembleClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a8c03",
   "metadata": {},
   "source": [
    "Dealing with Feature Importance\n",
    "\n",
    "Understanding which features contribute most to fraud detection is crucial. Easy Ensemble allows you to extract this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90fbbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the default AdaBoost inside Easy Ensemble\n",
    "importances = []\n",
    "for estimator in easy_ensemble.estimators_:\n",
    "    # Each estimator is an AdaBoost classifier\n",
    "    for base_estimator in estimator.estimators_:\n",
    "        # Extract feature importance from each base estimator (if it has this attribute)\n",
    "        if hasattr(base_estimator, 'feature_importances_'):\n",
    "            importances.append(base_estimator.feature_importances_)\n",
    "\n",
    "# Average importances across all base estimators\n",
    "if importances:\n",
    "    avg_importances = np.mean(importances, axis=0)\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': avg_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6479377",
   "metadata": {},
   "source": [
    "Adjusting the Decision Threshold\n",
    "\n",
    "For fraud detection, we often want to adjust the classification threshold to favor recall (detecting more frauds) at the cost of precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c92c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_pred_proba = model_with_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate precision and recall for different thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "\n",
    "# Choose a threshold that gives desired recall\n",
    "desired_recall = 0.95  # We want to catch 95% of frauds\n",
    "idx = np.argmin(np.abs(recall - desired_recall))\n",
    "selected_threshold = thresholds[idx]\n",
    "print(f\"Threshold for {desired_recall:.2f} recall: {selected_threshold:.3f}\")\n",
    "\n",
    "# Apply the new threshold\n",
    "y_pred_custom_threshold = (y_pred_proba >= selected_threshold).astype(int)\n",
    "\n",
    "# Evaluate with the new threshold\n",
    "print(\"\\nPerformance with custom threshold:\")\n",
    "print(classification_report(y_test, y_pred_custom_threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef445c",
   "metadata": {},
   "source": [
    "Real-time Processing Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a371747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code would be part of your production fraud detection system\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained models\n",
    "scaler = joblib.load('scaler_model.pkl')\n",
    "fraud_model = joblib.load('fraud_detection_model.pkl')\n",
    "\n",
    "def predict_fraud(transaction_features):\n",
    "    \"\"\"\n",
    "    Predict if a new blockchain transaction is fraudulent.\n",
    "    \n",
    "    Args:\n",
    "        transaction_features: Features of the transaction to check\n",
    "        \n",
    "    Returns:\n",
    "        fraud_probability: Probability of fraud\n",
    "        is_fraud: Boolean prediction\n",
    "    \"\"\"\n",
    "    # Preprocess features\n",
    "    features_scaled = scaler.transform([transaction_features])\n",
    "    \n",
    "    # Get fraud probability\n",
    "    fraud_probability = fraud_model.predict_proba(features_scaled)[0, 1]\n",
    "    \n",
    "    # Apply threshold (potentially adjusted based on business needs)\n",
    "    threshold = 0.7  # Higher threshold means fewer false positives\n",
    "    is_fraud = fraud_probability >= threshold\n",
    "    \n",
    "    return fraud_probability, is_fraud\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
